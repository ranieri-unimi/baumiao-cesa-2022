{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import warnings\n",
    "import imghdr\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.losses import *\n",
    "\n",
    "from keras.utils import image_dataset_from_directory, load_img, img_to_array\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT, DOG = 'Cats', 'Dogs'\n",
    "uri = 'downloads/{}/{}.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baumiao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (256, 256, 3) # (heigt, width, D)\n",
    "image_size = input_shape[:2]\n",
    "batch_size = 32\n",
    "N_channels = input_shape[2]\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_ext = set()\n",
    "ds = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1\n",
    "warnings.filterwarnings(\"error\")\n",
    "bad_paths = []\n",
    "\n",
    "for e in [CAT,DOG]:\n",
    "    for pth in Path(f'downloads/{e}').rglob(\"*\"):\n",
    "        \n",
    "        ext = imghdr.what(pth)\n",
    "        if ext is None:\n",
    "            print(pth,'removed')\n",
    "            os.remove(pth)\n",
    "        else:\n",
    "            exp_ext.add(ext)\n",
    "            try:\n",
    "                with Image.open(pth) as img:\n",
    "                    pxl = np.array(img)\n",
    "                    if pxl.ndim < 3 or pxl.shape[-1] < 3:\n",
    "                    # if pxl.ndim != 2:\n",
    "                        bad_paths.append(pth)\n",
    "                    else:\n",
    "                        pass\n",
    "            except Exception as e:\n",
    "                bad_paths.append(pth)\n",
    "                print(pth,e)\n",
    "\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2\n",
    "bad_paths = []\n",
    "shapeset = set()\n",
    "\n",
    "img_paths = glob.glob(os.path.join('downloads','*/*.*')) # assuming you point to the directory containing the label folders.\n",
    "for image_path in img_paths:\n",
    "\n",
    "    try:\n",
    "        img = load_img(image_path, target_size=image_size)\n",
    "        img = img_to_array(img)\n",
    "        shapeset.add(img.shape)\n",
    "        # img_bytes = tf.io.read_file(image_path)\n",
    "        # decoded_img = tf.decode_image(img_bytes)\n",
    "    except Exception as inst:\n",
    "        print('trouble at', image_path, ':', inst)\n",
    "        bad_paths.append(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 3\n",
    "bad_paths = []\n",
    "\n",
    "for folder_name in (\"Cats\", \"Dogs\"):\n",
    "    folder_path = os.path.join(\"downloads\", folder_name)\n",
    "    for fname in os.listdir(folder_path):\n",
    "        fpath = os.path.join(folder_path, fname)\n",
    "        \n",
    "        try:\n",
    "            fobj = open(fpath, \"rb\")\n",
    "            is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\n",
    "        finally:\n",
    "            fobj.close()\n",
    "        if not is_jfif:\n",
    "            print(f\"Found bad path {fpath}\")\n",
    "            bad_paths.append(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bad_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pth in bad_paths:\n",
    "    try:\n",
    "        os.remove(pth)\n",
    "        print(pth,'removed')\n",
    "    except Exception as e:\n",
    "        print('FATAL ERROR @', pth, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_dataset(x, y):\n",
    "#     imgs = []\n",
    "#     labels = []\n",
    "#     img_size = (128, 128)\n",
    "\n",
    "#     for i, j in zip(x, y):\n",
    "#         img = load_img(i, target_size=img_size)\n",
    "#         img = img_to_array(img)\n",
    "#         imgs.append(img)\n",
    "#         labels.append(j)\n",
    "#     imgs, labels = np.array(imgs), np.array(labels)\n",
    "#     return imgs, labels\n",
    "\n",
    "\n",
    "# x_train, y_train = make_dataset(\n",
    "#     [pth for e in [CAT,DOG] for pth in Path(f'downloads/{e}').rglob(\"*\") ],\n",
    "#     [e for e in [CAT,DOG] for pth in Path(f'downloads/{e}').rglob(\"*\")]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = image_dataset_from_directory(\n",
    "    \"downloads\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=42,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"binary\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = image_dataset_from_directory(\n",
    "    \"downloads\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=42,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"binary\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [DOG, CAT]\n",
    "N_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # AUTOTUNE = tf.data.AUTOTUNE\n",
    "# train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "# val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Rescaling(1.0 / 255, input_shape=input_shape),\n",
    "        Lambda(lambda x: (x ** 2)),\n",
    "\n",
    "        Conv2D(16, N_channels, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPooling2D(),\n",
    "\n",
    "        Conv2D(32, N_channels, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPooling2D(),\n",
    "\n",
    "        Conv2D(64, N_channels, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPooling2D(),\n",
    "\n",
    "        Conv2D(128, N_channels, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPooling2D(),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(256, activation=\"relu\"),\n",
    "\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        \n",
    "        Dense(8, activation=\"relu\"),\n",
    "\n",
    "        Dense(N_classes),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ths = 7\n",
    "acc = history.history['accuracy'][:ths]\n",
    "val_acc = history.history['val_accuracy'][:ths]\n",
    "\n",
    "loss = history.history['loss'][:ths]\n",
    "val_loss = history.history['val_loss'][:ths]\n",
    "\n",
    "epochs_range = range(ths)\n",
    "\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ecb3dd7b9afd5936190344805cde3b4d093a87e7de6d49ceea0a816cfd688af4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
